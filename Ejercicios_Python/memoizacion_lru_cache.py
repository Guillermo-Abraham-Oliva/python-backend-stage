import os
os.system('clear')

'''
ğŸ“Œ lru_cache es un decorador de la biblioteca estÃ¡ndar de Python (functools).
ğŸ“Œ Se usa para guardar en cachÃ© los resultados de una funciÃ³n y evitar recalcular valores repetidos.

@lru_cache(maxsize=None)
ğŸ“Œ Â¿QuÃ© hace?

Guarda los valores de las llamadas previas a la funciÃ³n en memoria (RAM).
Si la funciÃ³n se llama con un valor ya calculado, lo devuelve directamente desde la cachÃ© en lugar de recalcularlo.
maxsize=None significa que la cachÃ© no tiene lÃ­mite (guarda todos los resultados posibles) esto puede ser un ğŸ”´ Peligro en backend. Se recomienda usar un lÃ­mite: @lru_cache(maxsize=1000)


'''
from functools import lru_cache

@lru_cache(maxsize=None)
def fibonacci_cache(n):
    if n <= 0:
        return 0
    elif n == 1:
        return 1
    else:
        return fibonacci_cache(n-1) + fibonacci_cache(n-2)

# ğŸ“Œ Si n > 1, llama recursivamente a la funciÃ³n para calcular fibonacci(n-1) + fibonacci(n-2).
# ğŸ“Œ Gracias a @lru_cache, no vuelve a calcular los valores repetidos.

'''
âš ï¸ Â¿Se usa en backend profesional?
ğŸ”¹ En producciÃ³n NO se usa lru_cache porque la cachÃ©:

- Se borra al reiniciar el servidor.
- No es compartida entre mÃºltiples instancias en backend distribuido.

âœ… Alternativa moderna en backend: Usar Redis en FastAPI o Django.

ğŸ”¹   Uso en backend profesional: ---> 10%
âš ï¸ **Advertencia:** El decorador `@lru_cache` es Ãºtil para **memorizaciÃ³n en cÃ¡lculos repetitivos**, pero **en backend real se usan Redis o bases de datos cacheadas en lugar de esto**.  

âœ… **Ãšsalo para:**  
âœ”ï¸ Optimizar funciones de alto costo computacional en scripts pequeÃ±os.  

âŒ **EvÃ­talo cuando:**  
âŒ Necesites persistencia real de cachÃ© en backend (usa Redis, Memcached o FastAPI con `Depends`).  
âŒ Quieras manejar cachÃ© en mÃºltiples instancias de un servidor.  

ğŸ“Œ **ConclusiÃ³n:** Solo aprende lo justo sobre `@lru_cache`. **En backend real, usa Redis para almacenamiento en cachÃ© eficiente.**
'''